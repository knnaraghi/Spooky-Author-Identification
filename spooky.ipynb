{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# **Character-Level Classification using CNNs**\n",
    "\n",
    "For general background on this topic, check out the following link.\n",
    "\n",
    "[Best Practices for Document Classifcation with Deep Learning](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/)\n",
    "\n",
    "I am implementing the network described in this [paper](https://arxiv.org/pdf/1606.01781.pdf) which was also done by someone else in [this kernel](https://www.kaggle.com/robwec/character-level-author-identification-with-cnns) which I suggest you also check out. I found [this kernel](https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights) utilizing a CNN approach helpful as well. \n",
    "\n",
    "Overall, this is most likely not the best approach for this particular dataset but may be of use for others in the future tackling larger datasets. The authors in the paper linked above describe how this method does better on larger datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train = train.iloc[:,1].values\n",
    "y_train = train.iloc[:,2].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following block of code is adapted from [this repository](https://github.com/johnb30/py_crepe).\n",
    "\n",
    "This will take the sentences as input and turn each of them into a sparse character array. \n",
    "\n",
    "The max length is equivalent to the length of the sentence. 250 should be more than enough and you most likely can go even lower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "maxlen = 250\n",
    "alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n",
    "                list(string.punctuation) + ['\\n'])\n",
    "vocab_size = len(alphabet)\n",
    "check = set(alphabet)\n",
    "\n",
    "vocab = {}\n",
    "reverse_vocab = {}\n",
    "for ix, t in enumerate(alphabet):\n",
    "    vocab[t] = ix\n",
    "    reverse_vocab[ix] = t\n",
    "\n",
    "input_array = np.zeros((len(x_train), maxlen, vocab_size))\n",
    "for i, sentence in enumerate(x_train):\n",
    "    counter = 0\n",
    "    sentence_array = np.zeros((maxlen, vocab_size))\n",
    "    chars = list(sentence.lower().replace(' ', ''))\n",
    "    for c in chars:\n",
    "        if counter >= maxlen:\n",
    "            pass\n",
    "        else:\n",
    "            char_array = np.zeros(vocab_size, dtype=np.int)\n",
    "            if c in check:\n",
    "                ix = vocab[c]\n",
    "                char_array[ix] = 1\n",
    "            sentence_array[counter, :] = char_array\n",
    "            counter +=1\n",
    "    input_array[i, :, :] = sentence_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " ..., \n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "(19579, 250, 69)\n"
     ]
    }
   ],
   "source": [
    "print(input_array)\n",
    "print(np.shape(input_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "One-Hot Encoding of the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       ..., \n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "one_hot = LabelBinarizer()\n",
    "y_train = one_hot.fit_transform(y_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the architecture described in the paper linked to in the beginning. They describe 9-layer, 17-layer, 29-layer, and 49-layer variations but they all proceed in the same general manner as below. \n",
    "\n",
    "Do note that this method utilizes batch normalization instead of dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 250, 64)           13312     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 124, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 124, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 124, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 124, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 124, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 124, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 124, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 61, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 61, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 61, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 61, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 61, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 61, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 30, 512)           393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 30, 512)           786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 30, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              1050624   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 6,848,899\n",
      "Trainable params: 6,845,059\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "#from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same', input_shape=(250, 69)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=2))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=2))\n",
    "model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This alternate model below is the same as the one above but essentially doubled in size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 400, 64)           13312     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 400, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 400, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 400, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 400, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 400, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 400, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 400, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 400, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 199, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 199, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 199, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 199, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 199, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 199, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 199, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 199, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 199, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 99, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 99, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 99, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 99, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 99, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 99, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 99, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 99, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 99, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 99, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 99, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 99, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 49, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 49, 512)           393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 49, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 49, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 49, 512)           786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 49, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 49, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 49, 512)           786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 49, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 49, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 49, 512)           786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 49, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 49, 512)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2048)              1050624   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 8,947,459\n",
      "Trainable params: 8,939,779\n",
      "Non-trainable params: 7,680\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, padding='same', input_shape=(400, 69)))\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling1D(pool_size=3, strides=2))\n",
    "model2.add(Conv1D(filters=128, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=128, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=128, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=128, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling1D(pool_size=3, strides=2))\n",
    "model2.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling1D(pool_size=3, strides=2))\n",
    "model2.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(GlobalMaxPooling1D()) ###features = 512?\n",
    "model2.add(Dense(2048, activation='relu'))\n",
    "model2.add(Dense(2048, activation='relu'))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/60\n",
      "17s - loss: 1.1930 - acc: 0.4159 - val_loss: 1.1961 - val_acc: 0.3999\n",
      "Epoch 2/60\n",
      "9s - loss: 0.9588 - acc: 0.5349 - val_loss: 1.0099 - val_acc: 0.4612\n",
      "Epoch 3/60\n",
      "9s - loss: 0.8386 - acc: 0.6171 - val_loss: 0.9020 - val_acc: 0.5838\n",
      "Epoch 4/60\n",
      "9s - loss: 0.7272 - acc: 0.6819 - val_loss: 0.9003 - val_acc: 0.6121\n",
      "Epoch 5/60\n",
      "9s - loss: 0.6381 - acc: 0.7327 - val_loss: 1.0512 - val_acc: 0.5628\n",
      "Epoch 6/60\n",
      "9s - loss: 0.5541 - acc: 0.7716 - val_loss: 1.2110 - val_acc: 0.5365\n",
      "Epoch 7/60\n",
      "9s - loss: 0.4769 - acc: 0.8089 - val_loss: 1.5731 - val_acc: 0.5546\n",
      "Epoch 8/60\n",
      "9s - loss: 0.4118 - acc: 0.8353 - val_loss: 1.4723 - val_acc: 0.5429\n",
      "Epoch 9/60\n",
      "9s - loss: 0.3443 - acc: 0.8658 - val_loss: 1.5008 - val_acc: 0.5707\n",
      "Epoch 10/60\n",
      "9s - loss: 0.2866 - acc: 0.8888 - val_loss: 1.0211 - val_acc: 0.6359\n",
      "Epoch 11/60\n",
      "9s - loss: 0.2403 - acc: 0.9071 - val_loss: 1.4509 - val_acc: 0.6080\n",
      "Epoch 12/60\n",
      "9s - loss: 0.2122 - acc: 0.9183 - val_loss: 1.6534 - val_acc: 0.5585\n",
      "Epoch 13/60\n",
      "9s - loss: 0.1885 - acc: 0.9274 - val_loss: 3.1858 - val_acc: 0.4344\n",
      "Epoch 14/60\n",
      "9s - loss: 0.1517 - acc: 0.9434 - val_loss: 1.8259 - val_acc: 0.5804\n",
      "Epoch 15/60\n",
      "9s - loss: 0.1481 - acc: 0.9437 - val_loss: 2.9972 - val_acc: 0.4890\n",
      "Epoch 16/60\n",
      "9s - loss: 0.1347 - acc: 0.9488 - val_loss: 3.7300 - val_acc: 0.4441\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26373be6eb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=5, verbose=2, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "model.fit(input_array, y_train, validation_split=0.2, epochs=60, callbacks=callbacks_list, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Data Preparation of the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_test = test.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'Still, as I urged our leaving Ireland with such inquietude and impatience, my father thought it best to yield.'\n",
      " 'If a fire wanted fanning, it could readily be fanned with a newspaper, and as the government grew weaker, I have no doubt that leather and iron acquired durability in proportion, for, in a very short time, there was not a pair of bellows in all Rotterdam that ever stood in need of a stitch or required the assistance of a hammer.'\n",
      " 'And when they had broken down the frail door they found only this: two cleanly picked human skeletons on the earthen floor, and a number of singular beetles crawling in the shadowy corners.'\n",
      " ...,\n",
      " 'It is easily understood that what might improve a closely scrutinized detail, may at the same time injure a general or more distantly observed effect.'\n",
      " 'Be this as it may, I now began to feel the inspiration of a burning hope, and at length nurtured in my secret thoughts a stern and desperate resolution that I would submit no longer to be enslaved.'\n",
      " 'Long winded, statistical, and drearily genealogical as some of the matter was, there ran through it a continuous thread of brooding, tenacious horror and preternatural malevolence which impressed me even more than it had impressed the good doctor.']\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_array = np.zeros((len(x_test), maxlen, vocab_size))\n",
    "for i, sentence in enumerate(x_test):\n",
    "    counter = 0\n",
    "    sentence_array = np.zeros((maxlen, vocab_size))\n",
    "    chars = list(sentence.lower().replace(' ', ''))\n",
    "    for c in chars:\n",
    "        if counter >= maxlen:\n",
    "            pass\n",
    "        else:\n",
    "            char_array = np.zeros(vocab_size, dtype=np.int)\n",
    "            if c in check:\n",
    "                ix = vocab[c]\n",
    "                char_array[ix] = 1\n",
    "            sentence_array[counter, :] = char_array\n",
    "            counter +=1\n",
    "    test_array[i, :, :] = sentence_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " ..., \n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "(8392, 250, 69)\n"
     ]
    }
   ],
   "source": [
    "print(test_array)\n",
    "print(np.shape(test_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8392/8392 [==============================] - 3s       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict_proba(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ids = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(y_test, columns=['EAP', 'HPL', 'MWS'])\n",
    "submission.insert(0, \"id\", ids)\n",
    "submission.to_csv(\"my_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aind-cv]",
   "language": "python",
   "name": "conda-env-aind-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
